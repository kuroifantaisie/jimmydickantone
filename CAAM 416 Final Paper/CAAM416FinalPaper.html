<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">.lst-kix_o56iserptda4-2>li:before{content:"" counter(lst-ctn-kix_o56iserptda4-2,lower-roman) ". "}.lst-kix_o56iserptda4-0>li:before{content:"" counter(lst-ctn-kix_o56iserptda4-0,decimal) ". "}.lst-kix_o56iserptda4-4>li:before{content:"" counter(lst-ctn-kix_o56iserptda4-4,lower-latin) ". "}.lst-kix_o56iserptda4-1>li:before{content:"" counter(lst-ctn-kix_o56iserptda4-1,lower-latin) ". "}.lst-kix_o56iserptda4-5>li:before{content:"" counter(lst-ctn-kix_o56iserptda4-5,lower-roman) ". "}.lst-kix_o56iserptda4-7>li{counter-increment:lst-ctn-kix_o56iserptda4-7}.lst-kix_8miagtgqb53t-3>li{counter-increment:lst-ctn-kix_8miagtgqb53t-3}.lst-kix_o56iserptda4-6>li:before{content:"" counter(lst-ctn-kix_o56iserptda4-6,decimal) ". "}.lst-kix_o56iserptda4-7>li:before{content:"" counter(lst-ctn-kix_o56iserptda4-7,lower-latin) ". "}ol.lst-kix_8miagtgqb53t-6.start{counter-reset:lst-ctn-kix_8miagtgqb53t-6 0}.lst-kix_o56iserptda4-1>li{counter-increment:lst-ctn-kix_o56iserptda4-1}.lst-kix_8miagtgqb53t-4>li{counter-increment:lst-ctn-kix_8miagtgqb53t-4}.lst-kix_o56iserptda4-8>li:before{content:"" counter(lst-ctn-kix_o56iserptda4-8,lower-roman) ". "}ol.lst-kix_o56iserptda4-0.start{counter-reset:lst-ctn-kix_o56iserptda4-0 0}ol.lst-kix_8miagtgqb53t-0.start{counter-reset:lst-ctn-kix_8miagtgqb53t-0 0}ol.lst-kix_8miagtgqb53t-3.start{counter-reset:lst-ctn-kix_8miagtgqb53t-3 0}ol.lst-kix_o56iserptda4-3.start{counter-reset:lst-ctn-kix_o56iserptda4-3 0}ol.lst-kix_8miagtgqb53t-2.start{counter-reset:lst-ctn-kix_8miagtgqb53t-2 0}ol.lst-kix_o56iserptda4-1.start{counter-reset:lst-ctn-kix_o56iserptda4-1 0}.lst-kix_o56iserptda4-8>li{counter-increment:lst-ctn-kix_o56iserptda4-8}.lst-kix_8miagtgqb53t-1>li:before{content:"" counter(lst-ctn-kix_8miagtgqb53t-1,lower-latin) ". "}.lst-kix_8miagtgqb53t-2>li:before{content:"" counter(lst-ctn-kix_8miagtgqb53t-2,lower-roman) ". "}.lst-kix_o56iserptda4-2>li{counter-increment:lst-ctn-kix_o56iserptda4-2}ol.lst-kix_o56iserptda4-4.start{counter-reset:lst-ctn-kix_o56iserptda4-4 0}.lst-kix_8miagtgqb53t-3>li:before{content:"" counter(lst-ctn-kix_8miagtgqb53t-3,decimal) ". "}ol.lst-kix_o56iserptda4-7.start{counter-reset:lst-ctn-kix_o56iserptda4-7 0}.lst-kix_8miagtgqb53t-0>li:before{content:"" counter(lst-ctn-kix_8miagtgqb53t-0,decimal) ". "}.lst-kix_o56iserptda4-5>li{counter-increment:lst-ctn-kix_o56iserptda4-5}ol.lst-kix_o56iserptda4-2.start{counter-reset:lst-ctn-kix_o56iserptda4-2 0}ol.lst-kix_8miagtgqb53t-1.start{counter-reset:lst-ctn-kix_8miagtgqb53t-1 0}ol.lst-kix_8miagtgqb53t-6{list-style-type:none}ol.lst-kix_8miagtgqb53t-5{list-style-type:none}ol.lst-kix_8miagtgqb53t-4{list-style-type:none}ol.lst-kix_8miagtgqb53t-3{list-style-type:none}ol.lst-kix_8miagtgqb53t-2{list-style-type:none}.lst-kix_8miagtgqb53t-7>li:before{content:"" counter(lst-ctn-kix_8miagtgqb53t-7,lower-latin) ". "}ol.lst-kix_8miagtgqb53t-1{list-style-type:none}ol.lst-kix_8miagtgqb53t-0{list-style-type:none}.lst-kix_o56iserptda4-4>li{counter-increment:lst-ctn-kix_o56iserptda4-4}.lst-kix_8miagtgqb53t-5>li:before{content:"" counter(lst-ctn-kix_8miagtgqb53t-5,lower-roman) ". "}.lst-kix_8miagtgqb53t-6>li:before{content:"" counter(lst-ctn-kix_8miagtgqb53t-6,decimal) ". "}.lst-kix_8miagtgqb53t-6>li{counter-increment:lst-ctn-kix_8miagtgqb53t-6}.lst-kix_8miagtgqb53t-4>li:before{content:"" counter(lst-ctn-kix_8miagtgqb53t-4,lower-latin) ". "}ol.lst-kix_8miagtgqb53t-8{list-style-type:none}.lst-kix_8miagtgqb53t-1>li{counter-increment:lst-ctn-kix_8miagtgqb53t-1}ol.lst-kix_8miagtgqb53t-7{list-style-type:none}ol.lst-kix_o56iserptda4-5.start{counter-reset:lst-ctn-kix_o56iserptda4-5 0}ol.lst-kix_8miagtgqb53t-5.start{counter-reset:lst-ctn-kix_8miagtgqb53t-5 0}.lst-kix_8miagtgqb53t-7>li{counter-increment:lst-ctn-kix_8miagtgqb53t-7}.lst-kix_8miagtgqb53t-0>li{counter-increment:lst-ctn-kix_8miagtgqb53t-0}ol.lst-kix_8miagtgqb53t-8.start{counter-reset:lst-ctn-kix_8miagtgqb53t-8 0}.lst-kix_8miagtgqb53t-8>li:before{content:"" counter(lst-ctn-kix_8miagtgqb53t-8,lower-roman) ". "}ol.lst-kix_o56iserptda4-8{list-style-type:none}ol.lst-kix_o56iserptda4-8.start{counter-reset:lst-ctn-kix_o56iserptda4-8 0}ol.lst-kix_o56iserptda4-7{list-style-type:none}.lst-kix_o56iserptda4-0>li{counter-increment:lst-ctn-kix_o56iserptda4-0}ol.lst-kix_o56iserptda4-4{list-style-type:none}ol.lst-kix_o56iserptda4-3{list-style-type:none}ol.lst-kix_o56iserptda4-6{list-style-type:none}.lst-kix_o56iserptda4-3>li{counter-increment:lst-ctn-kix_o56iserptda4-3}ol.lst-kix_o56iserptda4-5{list-style-type:none}ol.lst-kix_8miagtgqb53t-7.start{counter-reset:lst-ctn-kix_8miagtgqb53t-7 0}ol.lst-kix_o56iserptda4-0{list-style-type:none}ol.lst-kix_o56iserptda4-2{list-style-type:none}ol.lst-kix_o56iserptda4-1{list-style-type:none}ol.lst-kix_o56iserptda4-6.start{counter-reset:lst-ctn-kix_o56iserptda4-6 0}.lst-kix_o56iserptda4-6>li{counter-increment:lst-ctn-kix_o56iserptda4-6}li.li-bullet-0:before{margin-left:-18pt;white-space:nowrap;display:inline-block;min-width:18pt}ol.lst-kix_8miagtgqb53t-4.start{counter-reset:lst-ctn-kix_8miagtgqb53t-4 0}.lst-kix_8miagtgqb53t-8>li{counter-increment:lst-ctn-kix_8miagtgqb53t-8}.lst-kix_8miagtgqb53t-2>li{counter-increment:lst-ctn-kix_8miagtgqb53t-2}.lst-kix_o56iserptda4-3>li:before{content:"" counter(lst-ctn-kix_o56iserptda4-3,decimal) ". "}.lst-kix_8miagtgqb53t-5>li{counter-increment:lst-ctn-kix_8miagtgqb53t-5}ol{margin:0;padding:0}table td,table th{padding:0}.c1{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Times New Roman";font-style:normal}.c6{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:10pt;font-family:"Times New Roman";font-style:normal}.c3{padding-top:0pt;padding-bottom:0pt;line-height:1.0;orphans:2;widows:2;text-align:center;height:11pt}.c2{padding-top:0pt;padding-bottom:0pt;line-height:1.0;orphans:2;widows:2;text-align:left;height:11pt}.c13{-webkit-text-decoration-skip:none;color:#1155cc;font-weight:400;text-decoration:underline;text-decoration-skip-ink:none;font-size:10pt;font-family:"Times New Roman"}.c9{padding-top:0pt;padding-bottom:0pt;line-height:1.0;orphans:2;widows:2;text-align:center}.c19{padding-top:12pt;padding-bottom:12pt;line-height:1.0;orphans:2;widows:2;text-align:left}.c5{background-color:#ffffff;font-size:13pt;font-family:"Times New Roman";font-style:italic;color:#303030;font-weight:400}.c4{padding-top:0pt;padding-bottom:0pt;line-height:1.0;orphans:2;widows:2;text-align:left}.c27{background-color:#ffffff;-webkit-text-decoration-skip:none;color:#1155cc;text-decoration:underline;text-decoration-skip-ink:none}.c14{color:#000000;text-decoration:none;vertical-align:baseline;font-style:normal}.c18{font-weight:700;font-size:14pt;font-family:"Times New Roman"}.c8{font-size:12pt;font-weight:400;font-family:"Times New Roman"}.c23{color:#000000;text-decoration:none;font-style:normal}.c15{font-size:10pt;font-weight:700;font-family:"Times New Roman"}.c0{font-size:12pt;font-weight:700;font-family:"Times New Roman"}.c24{font-weight:400;font-size:11pt;font-family:"Times New Roman"}.c21{text-decoration:none;vertical-align:baseline;font-style:normal}.c25{background-color:#ffffff;max-width:468pt;padding:72pt 72pt 72pt 72pt}.c16{font-size:13pt;font-family:"Times New Roman";font-weight:400}.c17{font-weight:400;font-size:10pt;font-family:"Times New Roman"}.c26{padding:0;margin:0}.c7{margin-left:36pt;padding-left:0pt}.c22{color:inherit;text-decoration:inherit}.c20{background-color:#ffffff;color:#303030}.c11{text-indent:36pt}.c10{vertical-align:super}.c12{vertical-align:sub}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c25 doc-content"><p class="c4"><span class="c1">Jimmy Dickantone</span></p><p class="c4"><span class="c1">4/25/2022</span></p><p class="c4"><span class="c1">CAAM 416</span></p><p class="c9"><span class="c1">Visualizing the Negative Afterimage Illusion</span></p><p class="c9"><span class="c1">(Project done in collaboration with Elizabeth Myers)</span></p><p class="c3"><span class="c1"></span></p><p class="c4"><span class="c14 c0">1. Introduction</span></p><p class="c2"><span class="c14 c18"></span></p><p class="c4"><span class="c8">An afterimage is a phenomenon described by the perception of an image after exposure to an original image is suspended. The most easily observable of these afterimages are negative afterimages, which are afterimages that appear inversely colored in comparison to the original image. When a negative afterimage is stared at for an extended period of time then subsequently switched with a desaturated version of the original image, an illusion can be observed in which the desaturated image is perceived as being normally colored. This is referred to as the negative afterimage illusion, and is mainly attributed to the adaptation of the photoreceptors within the retina.</span><span class="c8 c10">1</span><span class="c8">&nbsp;</span><span class="c1">To better understand the mechanisms behind this effect, this paper experiments using a model of local adaptation to visualize and observe the effects of differing variables on the perception of the negative afterimage illusion.</span></p><p class="c2"><span class="c1"></span></p><p class="c4"><span class="c14 c0">2. Physiological Mechanisms of the Negative Afterimage Illusion</span></p><p class="c2"><span class="c14 c0"></span></p><p class="c4"><span class="c8">Within the retina, light is mainly processed through two specialized types of neurons: rods and cones. These neurons are referred to collectively as photoreceptors, and are responsible for converting light in the form of photons into electrical signals for the brain to process.</span><span class="c8 c10">2</span><span class="c8">&nbsp;Rods contain only a single photopigment and can produce a response to a single photon of light, whereas cones exist in three types, each corresponding to a distinct wavelength of light (blue, green, or red). Although requiring approximately 100 photons required to produce a comparable response, due to the differing types of cones, they serve as the main proponent in the negative afterimage illusion.</span><span class="c8 c10">2</span><span class="c1">&nbsp;</span></p><p class="c2"><span class="c1"></span></p><p class="c4"><span class="c8">Both photoreceptors, however, &nbsp;contain the molecule opsin, which when exposed to photons, undergoes an exothermic chemical reaction that causes the ion channels of the photoreceptor&rsquo;s plasma membrane to close.</span><span class="c8 c10">3</span><span class="c8">&nbsp;The suspension of ion flow results in the hyperpolarization of the cell, which reduces the release of glutamate,</span><span class="c8 c10">3</span><span class="c8">&nbsp;an excitatory neurotransmitter center to the establishment of synaptic plasticity.</span><span class="c8 c10">4</span><span class="c8">&nbsp;This process, in which the illumination of the photoreceptors results in the reduced release of glutamate, is referred to as &ldquo;bleaching&rdquo;.</span><span class="c8 c10">3</span><span class="c1">&nbsp;</span></p><p class="c2"><span class="c1"></span></p><p class="c4"><span class="c8">When exposed to stimuli, the opsin within the photoreceptors undergo the bleaching process which, due to the reduction of glutamate release, results in desensitization of the photoreceptors to the wavelengths of light present in the stimuli.</span><span class="c8 c10">3</span><span class="c1">&nbsp;After prolonged exposure, these photoreceptors have essentially &ldquo;adapted&rdquo; to the stimuli, resulting in an increased sensitivity to the wavelengths of light that weren&rsquo;t present in the stimuli. This leads to the perception of an inversely colored image when presented with a desaturated version of the same image as stimuli, due to the increased sensitivity to the opposite wavelengths within the cones. Figure 1 illustrates such a process, and further demonstrates the adaptive effect of the photoreceptors.</span></p><p class="c2 c11"><span class="c1"></span></p><p class="c9 c11"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 402.00px; height: 185.00px;"><img alt="" src="images/image6.png" style="width: 402.00px; height: 325.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span class="c1">&nbsp; &nbsp; &nbsp; &nbsp;</span></p><p class="c9 c11"><span class="c6">(Figure 1: Demonstration of the Afterimage Effect within the Photoreceptors)</span></p><p class="c9 c11"><span class="c13"><a class="c22" href="https://www.google.com/url?q=https://oxford.universitypressscholarship.com/view/10.1093/acprof:oso/9780199936533.001.0001/acprof-9780199936533-chapter-10&amp;sa=D&amp;source=editors&amp;ust=1734618228217294&amp;usg=AOvVaw03LuO0bk8Dp0Tup6W22pGj">Adaptation - Oxford Scholarship (universitypressscholarship.com)</a></span></p><p class="c2 c11"><span class="c1"></span></p><p class="c2"><span class="c1"></span></p><p class="c4"><span class="c0">3. A Model of Local Adaptation</span></p><p class="c2"><span class="c1"></span></p><p class="c4"><span class="c8">&ldquo;A Model of Local Adaptation,&rdquo; Vangorp et. Al details the creation of a psychophysical measurement based model, in which experimental </span><span class="c8">stimuli are optimized in order to create a predictive model that best fits the data.</span><span class="c8 c10">4</span><span class="c8">&nbsp;Contrastingly to previous models, the experiment based determination of this model allows for better insight on the effects of spatial pooling within the adaptation process.</span><span class="c8 c10">4</span><span class="c8">&nbsp;This results in the discovery of a model that can reliably predict the steady state of optimization, and can be used in visualizations such as that of the negative afterimage illusion.</span></p><p class="c2 c11"><span class="c1"></span></p><p class="c4"><span class="c8">The first experiment, &ldquo;Probe-on-Flash,&rdquo; was done to see &ldquo;how the mismatch between background and adaptation luminance affects the visual performance across luminance levels&rdquo;.</span><span class="c8 c10">4 </span><span class="c8">Individuals were first made to look at and adapt to a background luminance (L</span><span class="c8 c12">f</span><span class="c8">) for around 1 to 3 minutes.</span><span class="c8 c10">4</span><span class="c8">&nbsp;They were then flashed with an edge with a pedestal detection luminance (L</span><span class="c8 c12">p</span><span class="c8">) for around 200 ms,</span><span class="c8">&nbsp;and required to determine whether it was oriented horizontally or vertically.</span><span class="c8 c10">4</span><span class="c8">&nbsp;For each background luminance, the contrast was measured, defined in this case as the detection threshold (&Delta;L) divided by the background luminance.</span><span class="c8 c10">4</span><span class="c8">&nbsp;This data was plotted (figure 2) alongside the prediction given by the Naka-Rushton model derived as a function of the adaptation of single receptors.</span><span class="c8 c10">4</span><span class="c1">&nbsp;</span></p><p class="c2"><span class="c1"></span></p><p class="c9 c11"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 225.00px; height: 161.00px;"><img alt="" src="images/image19.png" style="width: 1039.26px; height: 582.59px; margin-left: -187.37px; margin-top: -194.20px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c9 c11"><span class="c6">(Figure 2: Detection Contrast Plotted Against Background Luminance)</span></p><p class="c9 c11"><span class="c14 c17">The dotted lines represent the predictions done by the Naka-Rushton model, whereas the solid lines represent the experimental data. The black solid line represents the tvi function plotted over luminance.</span></p><p class="c9 c11"><span class="c13"><a class="c22" href="https://www.google.com/url?q=https://resources.mpi-inf.mpg.de/LocalAdaptation/LocalAdaptation.pdf&amp;sa=D&amp;source=editors&amp;ust=1734618228218938&amp;usg=AOvVaw34ObWadTLCk1whLJ-RpLzI">A Model of Local Adaptation (mpg.de)</a></span></p><p class="c2 c11"><span class="c1"></span></p><p class="c4"><span class="c8">The Naka-Rushton equation is described as below:</span></p><p class="c9 c11"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 194.50px; height: 58.96px;"><img alt="" src="images/image5.png" style="width: 1441.38px; height: 796.90px; margin-left: -889.17px; margin-top: -676.09px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c4"><span class="c1">Where where L is the flash luminance, n is a constant between 0.7 and 1,</span></p><p class="c4"><span class="c8">and k is a scaling constant, and &sigma;(La) is the half-saturation constant. Further, the contrast sensitivity function (CSF) can be used to predict sensitivity according to the following equation</span><span class="c8 c10">4</span><span class="c8">:</span></p><p class="c9 c11"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 179.00px; height: 47.00px;"><img alt="" src="images/image5.png" style="width: 1366.56px; height: 767.24px; margin-left: -240.90px; margin-top: -314.77px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c4"><span class="c1">Where threshold versus intensity (tvi) is additionally derived in terms of the CSF:</span></p><p class="c9 c11"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 202.00px; height: 43.10px;"><img alt="" src="images/image5.png" style="width: 1369.95px; height: 767.24px; margin-left: -466.55px; margin-top: -319.77px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c4"><span class="c8">To best fit the data, tvi was determined to be based not on adaptive luminance, but retinal luminance, which can be calculated through the convolution of I (the luminance image that enters the retina) with a point spread function due to glare (O)</span><span class="c8 c10">4</span><span class="c1">:</span></p><p class="c9 c11"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 87.79px; height: 34.68px;"><img alt="" src="images/image5.png" style="width: 1442.21px; height: 804.08px; margin-left: -935.99px; margin-top: -424.08px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c4"><span class="c8">To account for maladaptation, the Naka-Ruston equation is again used, this time with the semi-saturation set equal to that of the adaptive luminance, to align with the experimental observations of response being strongest when the eye is fully adapted to background luminance.</span><span class="c8 c10">4</span><span class="c8">&nbsp;As photoreceptors have the strongest responses to smaller detection thresholds, this leads to the conclusion that change in response being inversely proportional to detection threshold.</span><span class="c8 c10">4</span><span class="c8">&nbsp;Represented as</span><span class="c8 c10">4</span><span class="c1">:</span></p><p class="c2"><span class="c1"></span></p><p class="c9 c11"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 72.50px; height: 45.00px;"><img alt="" src="images/image16.png" style="width: 1284.12px; height: 752.14px; margin-left: -393.80px; margin-top: -315.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c3 c11"><span class="c1"></span></p><p class="c4"><span class="c8">This equation can be combined with the derivative of the Naka-Rushton equation to create</span><span class="c8 c10">4</span><span class="c1">:</span></p><p class="c9 c11"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 174.00px; height: 51.46px;"><img alt="" src="images/image16.png" style="width: 1302.00px; height: 763.08px; margin-left: -348.64px; margin-top: -404.12px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c4"><span class="c8">With the threshold elevation (the point of complete adaptation) being given by </span><span class="c8 c10">4</span><span class="c1">:</span></p><p class="c9 c11"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 280.00px; height: 51.00px;"><img alt="" src="images/image16.png" style="width: 1302.00px; height: 756.24px; margin-left: -296.64px; margin-top: -513.50px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c3 c11"><span class="c1"></span></p><p class="c4"><span class="c8">Using the predictions given by the model, the detection threshold can then be determined as following this equation</span><span class="c8 c10">4</span><span class="c1">:</span></p><p class="c2"><span class="c1"></span></p><p class="c9 c11"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 272.49px; height: 50.50px;"><img alt="" src="images/image2.png" style="width: 1477.41px; height: 844.07px; margin-left: -357.09px; margin-top: -444.88px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 492.55px; height: 146.14px;"><img alt="" src="images/image15.png" style="width: 1125.82px; height: 636.10px; margin-left: -92.01px; margin-top: -274.30px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c9 c11"><span class="c15">(Figure 3: Detection Model Diagram)</span></p><p class="c9 c11"><span class="c13"><a class="c22" href="https://www.google.com/url?q=https://resources.mpi-inf.mpg.de/LocalAdaptation/LocalAdaptation.pdf&amp;sa=D&amp;source=editors&amp;ust=1734618228221381&amp;usg=AOvVaw0VtbMoUDg8UGRylLrdRGvE">A Model of Local Adaptation (mpg.de)</a></span></p><p class="c3"><span class="c1"></span></p><p class="c4"><span class="c8">These equations can be combined to form the model shown in Figure 3.</span><span class="c8 c10">4</span><span class="c8">&nbsp;However, it is important to note that the mechanism for Local Adaptation is still yet to be determined for complex images. In order to help identify an appropriate function, six more experiments were performed measuring frequency selectivity, the extent of visual area that influences adaptation, long-range lumination effects on adaptation, nonlinear pooling, orientation and contrast masking, and measuring the detection thresholds of more complex images.</span><span class="c8 c10">4</span><span class="c1">&nbsp;The results of these experiments were used for the creation of the adaptation models seen in Table 1.</span></p><p class="c2"><span class="c1"></span></p><p class="c9"><span class="c6">Table 1: Potential Local Adaptation Models</span></p><p class="c9"><span class="c14 c17">p represents the OTF model used and g represents the gaussian convolution used for spatial pooling.</span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 310.24px; height: 166.47px;"><img alt="" src="images/image1.png" style="width: 1257.07px; height: 703.87px; margin-left: -666.81px; margin-top: -126.25px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2"><span class="c1"></span></p><p class="c4"><span class="c8">Due to the number of unique combinations possible using differing optical transfer functions (OTFs), 56 potential models were created.</span><span class="c8 c10">4</span><span class="c8">&nbsp;These models were ranked in order of increasing &#43865;</span><span class="c8 c10">2</span><span class="c8 c12">red </span><span class="c1">values, determined by the following equation:</span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 225.25px; height: 63.82px;"><img alt="" src="images/image1.png" style="width: 1580.66px; height: 883.88px; margin-left: -913.96px; margin-top: -729.44px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c4"><span class="c8">Where &ldquo;N is the number of fitted stimuli, d is the number of degrees of freedom (free parameters), oi is the measurement, mi is the model prediction&hellip; (and) &sigma; is the standard error, which is due to both within and between-observer variations&#39;&#39;.</span><span class="c8 c10">4</span><span class="c8">&nbsp;When the models were then tested with 10 more stimuli made to maximize the difference in predicted detection thresholds of the models, the rankings of the models change as seen in table 2.</span><span class="c8 c10 c23">4</span></p><p class="c2"><span class="c8 c23 c10"></span></p><p class="c9"><span class="c15">Table 2: Potential Local Adaptation Models After Tested with Stimuli</span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 330.43px; height: 163.50px;"><img alt="" src="images/image4.png" style="width: 1241.02px; height: 701.18px; margin-left: -288.09px; margin-top: -434.69px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c3"><span class="c1"></span></p><p class="c4"><span class="c8">This reveals a new best fit model, which can be expressed as </span><span class="c8 c10">4</span><span class="c1">:</span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 347.00px; height: 30.71px;"><img alt="" src="images/image4.png" style="width: 1443.20px; height: 831.51px; margin-left: -754.22px; margin-top: -233.06px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c4"><span class="c8">Where &ldquo;&alpha; = 0.654, &lowast; is the convolution operator and the parameters for the Gaussian kernels g are &sigma;1 = 0.428&#9702; and &sigma;2 = 0.0824&#9702; (and) n1 and n2 are custom non-linearities&rdquo;,</span><span class="c8">&nbsp;given by the equation</span><span class="c8 c10">4</span><span class="c1">:</span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 121.50px; height: 59.00px;"><img alt="" src="images/image4.png" style="width: 1446.23px; height: 832.42px; margin-left: -880.32px; margin-top: -332.48px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c4"><span class="c1">This adaptation model can be represented by the diagram in figure 4, and be implemented using the MatLab code provided by the paper.</span></p><p class="c2"><span class="c1"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 308.31px; height: 157.47px;"><img alt="" src="images/image7.png" style="width: 1034.32px; height: 580.71px; margin-left: -545.34px; margin-top: -133.07px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c9 c11"><span class="c15">(Figure 3: Adaptation Model Diagram)</span></p><p class="c9 c11"><span class="c13"><a class="c22" href="https://www.google.com/url?q=https://resources.mpi-inf.mpg.de/LocalAdaptation/LocalAdaptation.pdf&amp;sa=D&amp;source=editors&amp;ust=1734618228223781&amp;usg=AOvVaw1XhZ96bEXMdbU_2wQe6E5_">A Model of Local Adaptation (mpg.de)</a></span></p><p class="c3 c11"><span class="c1"></span></p><p class="c4"><span class="c14 c0">4. Materials and Methods</span></p><p class="c2"><span class="c1"></span></p><p class="c4"><span class="c1">For our project, we decided to visualize the result of the negative afterimage illusion when either length of exposure, inverted image luminance, or the spatial position of the image were varied. The base image used for each of these visualizations was a colored image of Lovett Hall at Rice University, as seen in figure 4. Further, these visualizations were done in the form of the generation of an image through the use of MatLab, in order to recreate a subject&rsquo;s perception of said image.</span></p><p class="c2"><span class="c1"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 527.00px; height: 273.18px;"><img alt="" src="images/image13.jpg" style="width: 527.00px; height: 273.18px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c9"><span class="c15">(Figure 4: Lovett Hall at Rice University)</span></p><p class="c3 c11"><span class="c1"></span></p><p class="c4"><span class="c1">To properly tune our computational models, we ran experiments with the two of us as the subjects, in which we observed the illusions ourselves. In order to experience the illusion, we first needed to create a desaturated version and an inverted version of the image from Figure 4. The creation of the inverted image was not done traditionally, in that we first used the fill tool within photoshop to modify the shadows of the original image before inverting it. In this way, we would be able to get better results when going through with the experiment. The desaturated image was then created by simply removing all saturation from the image in photoshop. After adding a white dot to the center of each of the images to make it easier to focus, we were able to produce the images seen in figures 5 and 6. </span></p><p class="c2"><span class="c1"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 285.27px; height: 148.60px;"><img alt="" src="images/image3.png" style="width: 285.27px; height: 148.60px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 283.60px; height: 146.50px;"><img alt="" src="images/image8.png" style="width: 283.60px; height: 146.50px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c3"><span class="c1"></span></p><p class="c9"><span class="c6">(Figures 5 and 6: Inverted and Desaturated Images of Lovett Hall)</span></p><p class="c3"><span class="c6"></span></p><p class="c4"><span class="c14 c0">4.1 Time of Exposure</span></p><p class="c4"><span class="c8">(The code for the visualizations is attached as an m file and pdf)</span></p><p class="c2"><span class="c14 c0"></span></p><p class="c4"><span class="c8">To experiment with time of exposure, the subject was required to stare at the inverted image for a fixed period of time, then change their gaze to the desaturated image, and report on how bright the image appeared. The computational model of this effect was done using an equation based on the results of &ldquo;The timescale of adaptation at early and mid-level stages of visual processing,&rdquo; Gaoxing Mei et al.</span><span class="c8 c10">5</span><span class="c8">&nbsp;Within the paper, they described the adaptation period using the following model</span><span class="c8 c10">5</span><span class="c1">:</span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 163.58px; height: 53.26px;"><img alt="" src="images/image22.png" style="width: 1186.89px; height: 662.64px; margin-left: -334.76px; margin-top: -357.84px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c4"><span class="c1">For use within our visualization, we interpreted the function as determining the percent brightness of the original image perceived. We make use of the Vangorp et. al linear adaptation model to generate a desaturated version of the original image that accounts for blur and color loss. </span></p><p class="c2"><span class="c1"></span></p><p class="c4"><span class="c8">Within MatLab, a colored image can be generated by subtracting the inverted image from the desaturated image. However, this results in an image that is very brightly colored. To retain the effects of color loss as seen in the linear adaptation model, we multiplied the inverted image by a constant that resulted in a generated image similar to that of the one in figure 4. We used this constant for our value of y</span><span class="c8 c12">0</span><span class="c8">&nbsp;= 0.15, where the y value is multiplied by the inverted image. Further, for the time constant, we went with &#120591; = 7, as this leads to a y value that is near y</span><span class="c8 c12">0</span><span class="c1">&nbsp;for exposure times starting of around 30 seconds- where we estimated maximum adaptation would be reached- and greater. The images were then plotted to observe the effects of exposure time on the illusion and compared with the experimental results.</span></p><p class="c2"><span class="c1"></span></p><p class="c3"><span class="c6"></span></p><p class="c4"><span class="c0">4.2 Luminance of Inverted Image</span></p><p class="c2"><span class="c1"></span></p><p class="c4"><span class="c1">For the experimental portion of the luminance visualization, a subject was required to first stare at an inverted image for 30 seconds, then immediately look at a desaturated version of the original image, reporting on how bright the colors in the perceived image appeared. This procedure was done over multiple trials, in which the luminance of the inverted image was increased for each trial.</span></p><p class="c2"><span class="c1"></span></p><p class="c4"><span class="c1">The computational model of this visualization followed a similar process to that of 4.1, in which we generated a desaturated version of the image using the Vangorp et. al linear adaptation model. Instead of the inverted image, however, we modified the luminance of the original image in order to reduce computational intensity. We then performed this process using uint8 matrices within matlab:</span></p><p class="c2"><span class="c1"></span></p><p class="c9"><span class="c16">I</span><span class="c16 c12">lum</span><span class="c16">&nbsp;= I - I</span><span class="c16 c12">desat</span><span class="c16">&nbsp;+ L</span><span class="c12 c16">a</span></p><p class="c3"><span class="c14 c16"></span></p><p class="c4"><span class="c8">Where I represents the image with increased luminance, I</span><span class="c8 c12">desat</span><span class="c8">&nbsp;represents the desaturated version of that image, and L</span><span class="c8 c12">a</span><span class="c1">&nbsp;represents the adapted luminance generated by the linear adaptation model. Using a function that we created to modify the luminance of the image, we performed this transformation at differing % luminance increases.</span></p><p class="c2"><span class="c1"></span></p><p class="c4"><span class="c0">4.3</span><span class="c8">&nbsp;</span><span class="c14 c0">Spatial Position Shifting</span></p><p class="c2"><span class="c1"></span></p><p class="c4"><span class="c1">The experimental portion of the final visualization required the subject to stare at the inverted image (Figure 5) for 30 seconds and subsequently look at a desaturated version of the same image, but shifted a few units. Figure 7 shows this shifted image, which was generated by shifting the desaturated image to the right of the canvas, and pasting a copy of the left side of the image in the blank space created. The white dot is kept in the center of the canvas in order to maintain the subject&rsquo;s focus in the same spot.</span></p><p class="c2"><span class="c1"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 274.68px; height: 143.70px;"><img alt="" src="images/image3.png" style="width: 274.68px; height: 143.70px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 277.51px; height: 144.30px;"><img alt="" src="images/image17.png" style="width: 277.51px; height: 144.30px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c9"><span class="c6">(Figure 7: Shifted Desaturated Image of Lovett Hall at Rice University)</span></p><p class="c9"><span class="c14 c17">Figure 5 is shown alongside Figure 7 to allow for the observation of the illusion.</span></p><p class="c3"><span class="c6"></span></p><p class="c4"><span class="c1">For the computational aspect, the linear adaptation model was used to generate a new desaturated version of the image seen in Figure 7. The original image was then modified using the same process as in the equation from 4.2 in order to produce the visualization.</span></p><p class="c2"><span class="c1"></span></p><p class="c4"><span class="c0">5</span><span class="c8">&nbsp;</span><span class="c0">Results and Visualizations</span></p><p class="c2"><span class="c1"></span></p><p class="c4"><span class="c1">For the first visualization, the perceived images with t = 0,5,10,30 s of exposure are shown respectively. </span></p><p class="c4"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 299.03px; height: 248.50px;"><img alt="" src="images/image20.png" style="width: 299.03px; height: 248.50px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 302.03px; height: 251.11px;"><img alt="" src="images/image14.png" style="width: 302.03px; height: 251.11px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c4"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 298.50px; height: 248.41px;"><img alt="" src="images/image11.png" style="width: 298.50px; height: 248.41px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 296.50px; height: 247.08px;"><img alt="" src="images/image18.png" style="width: 296.50px; height: 247.08px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2"><span class="c1"></span></p><p class="c2"><span class="c1"></span></p><p class="c4"><span class="c1">It can be seen that although there is an increase in the brightness of the afterimage illusion when exposure time is increased, the difference between 0-5 seconds is much greater than that between 10-30 seconds, explained due to the exponential nature of the model used for time adaptation. This aligns with the observations we made within our experiments, of an increased time of exposure resulting in brighter results, with a significant drop off of this effect for exposure times longer than the 5 second mark.</span></p><p class="c2"><span class="c1"></span></p><p class="c4"><span class="c1">The computational visualization of the change in luminance is demonstrated at a 0% , 10%, 25%, and 50% luminance increase respectively. </span></p><p class="c2"><span class="c1"></span></p><p class="c4"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 304.69px; height: 254.07px;"><img alt="" src="images/image10.png" style="width: 304.69px; height: 254.07px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 308.66px; height: 255.07px;"><img alt="" src="images/image10.png" style="width: 308.66px; height: 255.07px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c4"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 293.03px; height: 246.50px;"><img alt="" src="images/image21.png" style="width: 293.03px; height: 246.50px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 296.50px; height: 246.91px;"><img alt="" src="images/image9.png" style="width: 296.50px; height: 246.91px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2"><span class="c1"></span></p><p class="c4"><span class="c1">The results of the computational visualization start to differ with the experimental observations with luminance increase percentages past 10%. Within the last two images, we can observe images that are brighter than the original image, which is not something that we observed within any of the experimental trials.</span></p><p class="c2"><span class="c1"></span></p><p class="c2"><span class="c14 c0"></span></p><p class="c4"><span class="c1">For the final visualization, we were able to generate the following image:</span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 509.55px; height: 271.92px;"><img alt="" src="images/image12.png" style="width: 509.55px; height: 271.92px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c3"><span class="c1"></span></p><p class="c4"><span class="c1">This result aligns with our experimental observations, demonstrating the important spatial properties within local adaptation, as we see parts of the building still appearing in black and white, and other parts containing color.</span></p><p class="c2"><span class="c1"></span></p><p class="c4"><span class="c0">6 Discussion</span></p><p class="c2"><span class="c1"></span></p><p class="c4"><span class="c1">The computational model we used to demonstrate the time adaptation of the photoreceptors may not correctly represent the physiological process that occurs, but demonstrates results similar to that observed when performing the experiment. The way we generated the images within MatLab however is similar to the process of negative feedback due to bleaching, with the generation of a colored image from the subtraction of an inverted image from a black and white image. In the future, we could attempt to use a more robust method to generate the image, such as not using uint8 integers (which must be between 0 and 255), and pursuing a more biologically inferred time adaptation model.</span></p><p class="c2"><span class="c1"></span></p><p class="c4"><span class="c1">The visualization of changing luminance is what appears to be most inaccurate out of the three visualizations, and this may be due to the fact that we only use the linear adaptation model once on the normal image, instead of using it on the inverted image. This may be the cause of the generated image that seems to perpetually increase in luminance each time the luminance of the inverted image is increased, which is not something that we observed within experimentation. Future improvements could include the use of a more accurate equation to generate the image, and passing the saturated image through the adaptation model instead.</span></p><p class="c2"><span class="c1"></span></p><p class="c4"><span class="c1">The visualization of the shifted image produced results that were very similar to the experimental results. Although the image appeared to be brighter than the one observed experimentally, this may be due to the fact that this afterimage tended to disappear much faster than the other types. The physiological mechanism behind that process may be explained by the halt of glutamate production due to eye movement, which would result in a quick disappearance of the afterimage. To better refine this process, the experiment could be done in a more controlled environment in order to better visualize the shifted afterimage, and the MatLab implementation could additionally be modified. Further, within all of the experiments we could benefit from a larger sample size of subjects, and a more objective way to quantify our results.</span></p><p class="c2"><span class="c1"></span></p><p class="c3"><span class="c1"></span></p><p class="c3"><span class="c1"></span></p><p class="c3"><span class="c1"></span></p><p class="c3"><span class="c1"></span></p><p class="c3"><span class="c1"></span></p><p class="c3"><span class="c1"></span></p><p class="c3"><span class="c1"></span></p><p class="c3"><span class="c1"></span></p><p class="c3"><span class="c1"></span></p><p class="c3"><span class="c1"></span></p><p class="c3"><span class="c1"></span></p><p class="c3"><span class="c1"></span></p><p class="c3"><span class="c1"></span></p><p class="c3"><span class="c1"></span></p><p class="c3"><span class="c1"></span></p><p class="c3"><span class="c1"></span></p><p class="c3"><span class="c1"></span></p><p class="c3"><span class="c1"></span></p><p class="c3"><span class="c1"></span></p><p class="c3"><span class="c1"></span></p><p class="c3"><span class="c1"></span></p><p class="c3"><span class="c1"></span></p><p class="c9"><span class="c8">Works Cited</span></p><p class="c3"><span class="c1"></span></p><ol class="c26 lst-kix_o56iserptda4-0 start" start="1"><li class="c4 c7 li-bullet-0"><span class="c16 c20">Li, H., Liu, X., Andolina, I. M., Li, X., Lu, Y., Spillmann, L., &amp; Wang, W. (2017). Asymmetries of Dark and Bright Negative Afterimages Are Paralleled by Subcortical ON and OFF Poststimulus Responses. </span><span class="c5">The Journal of neuroscience : the official journal of the Society for Neuroscience</span><span class="c16 c20">, </span><span class="c5">37</span><span class="c16 c20">(8), 1984&ndash;1996. </span><span class="c16 c27"><a class="c22" href="https://www.google.com/url?q=https://doi.org/10.1523/JNEUROSCI.2021-16.2017&amp;sa=D&amp;source=editors&amp;ust=1734618228231969&amp;usg=AOvVaw037jrT67XrOxKp5fmQ8lZs">https://doi.org/10.1523/JNEUROSCI.2021-16.2017</a></span></li><li class="c4 c7 li-bullet-0"><span class="c21 c16 c20">Neuroscience, 3rd Edition,Marc W. Halterman, Neurology Feb 2005, 64 (4) 769-769-a; DOI: 10.1212/01.WNL.0000154473.43364.47</span></li><li class="c7 c19 li-bullet-0"><span class="c16 c20">Schiller, P. H., &amp; Tehovnik, E. J. (n.d.). </span><span class="c5">Adaptation</span><span class="c21 c16 c20">. Oxford Scholarship Online. Retrieved April 27, 2022, from https://oxford.universitypressscholarship.com/view/10.1093/acprof:oso/9780199936533.001.0001/acprof-9780199936533-chapter-10 </span></li><li class="c19 c7 li-bullet-0"><span class="c5">The role of glutamate in the healthy brain and in the pathophysiology of parkinson&#39;s disease</span><span class="c21 c16 c20">. touchNEUROLOGY. (2019, November 12). Retrieved April 27, 2022, from https://touchneurology.com/parkinsons-disease/journal-articles/the-role-of-glutamate-in-the-healthy-brain-and-in-the-pathophysiology-of-parkinsons-disease-2/#article </span></li><li class="c19 c7 li-bullet-0"><span class="c5">A model of local adaptation ACM Transactions on Graphics (Proceedings of ACM SIGGRAPH Asia 2015)</span><span class="c16 c20 c21">. A Model of Local Adaptation. (n.d.). Retrieved April 27, 2022, from https://resources.mpi-inf.mpg.de/LocalAdaptation/ </span></li><li class="c4 c7 li-bullet-0"><span class="c21 c16 c20">Gaoxing Mei, Xue Dong, Min Bao; The timescale of adaptation at early and mid-level stages of visual processing. Journal of Vision 2017;17(1):1. doi: https://doi.org/10.1167/17.1.1.</span></li></ol><p class="c2"><span class="c1"></span></p></body></html>